{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYAh6qr2tNA9",
        "outputId": "dfc4aeac-8d41-4c32-ac67-c8ca6a115619"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Training dataset loaded successfully.\n",
            "Dataset shape: (1981520, 79)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv('training_set.csv')\n",
        "    print(\"✅ Training dataset loaded successfully.\")\n",
        "    print(f\"Dataset shape: {df.shape}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"❌ Error: 'training_set.csv' not found. Please place it in the correct directory.\")\n",
        "    exit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vv6HRptyuNxn",
        "outputId": "529f5fea-4272-4cde-a6a0-f3b1293645ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Missing values per column:\n",
            " Destination Port              0\n",
            " Flow Duration                 0\n",
            " Total Fwd Packets             0\n",
            " Total Backward Packets        0\n",
            "Total Length of Fwd Packets    0\n",
            "                              ..\n",
            "Idle Mean                      0\n",
            " Idle Std                      0\n",
            " Idle Max                      0\n",
            " Idle Min                      0\n",
            " Label                         0\n",
            "Length: 79, dtype: int64\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3970738545.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[col].fillna(df[col].mean(), inplace=True)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"\\nMissing values per column:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "for col in df.select_dtypes(include=np.number).columns:\n",
        "    df[col].fillna(df[col].mean(), inplace=True)\n",
        "\n",
        "X = df.drop(' Label', axis=1)\n",
        "y = df[' Label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBa5xCQGudc9",
        "outputId": "0706ce9d-3b04-4bb0-c397-ed8b2c68ffc5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2320834879.py:8: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X[col].fillna(X[col].median(), inplace=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Features have been normalized using Min-Max scaling (range 0 to 1).\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler \n",
        "\n",
        "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "for col in X.columns:\n",
        "    if X[col].isnull().any():\n",
        "        X[col].fillna(X[col].median(), inplace=True)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(\"✅ Features have been normalized using Min-Max scaling (range 0 to 1).\")\n",
        "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 840
        },
        "id": "amkFXhnnvPqn",
        "outputId": "9e5f6989-115b-4742-de29-13753f25b65c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Calculated class weights: {0: np.float64(0.6226620947630923), 1: np.float64(2.5381194409148664)}\n",
            "This will penalize errors on the minority class more heavily.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,056</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m5,056\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,169</span> (28.00 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,169\u001b[0m (28.00 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,169</span> (28.00 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m7,169\u001b[0m (28.00 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Starting Model Training with Early Stopping and Checkpointing ---\n",
            "Epoch 1/5\n",
            "\u001b[1m49520/49538\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9302 - loss: 0.1543 - precision_2: 0.7634 - recall_2: 0.9534\n",
            "Epoch 1: val_loss improved from inf to 0.07100, saving model to best_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m49538/49538\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 4ms/step - accuracy: 0.9302 - loss: 0.1543 - precision_2: 0.7634 - recall_2: 0.9534 - val_accuracy: 0.9724 - val_loss: 0.0710 - val_precision_2: 0.8909 - val_recall_2: 0.9801\n",
            "Epoch 2/5\n",
            "\u001b[1m49534/49538\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9646 - loss: 0.0893 - precision_2: 0.8635 - recall_2: 0.9743\n",
            "Epoch 2: val_loss did not improve from 0.07100\n",
            "\u001b[1m49538/49538\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 4ms/step - accuracy: 0.9646 - loss: 0.0893 - precision_2: 0.8635 - recall_2: 0.9743 - val_accuracy: 0.9683 - val_loss: 0.0776 - val_precision_2: 0.8687 - val_recall_2: 0.9884\n",
            "Epoch 3/5\n",
            "\u001b[1m49522/49538\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9683 - loss: 0.0808 - precision_2: 0.8744 - recall_2: 0.9789\n",
            "Epoch 3: val_loss improved from 0.07100 to 0.06818, saving model to best_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m49538/49538\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 4ms/step - accuracy: 0.9683 - loss: 0.0808 - precision_2: 0.8744 - recall_2: 0.9789 - val_accuracy: 0.9770 - val_loss: 0.0682 - val_precision_2: 0.9036 - val_recall_2: 0.9886\n",
            "Epoch 4/5\n",
            "\u001b[1m49522/49538\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9703 - loss: 0.0767 - precision_2: 0.8816 - recall_2: 0.9807\n",
            "Epoch 4: val_loss did not improve from 0.06818\n",
            "\u001b[1m49538/49538\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 4ms/step - accuracy: 0.9703 - loss: 0.0767 - precision_2: 0.8816 - recall_2: 0.9807 - val_accuracy: 0.9683 - val_loss: 0.0751 - val_precision_2: 0.8694 - val_recall_2: 0.9875\n",
            "Epoch 5/5\n",
            "\u001b[1m49530/49538\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9710 - loss: 0.0738 - precision_2: 0.8839 - recall_2: 0.9822\n",
            "Epoch 5: val_loss improved from 0.06818 to 0.06514, saving model to best_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m49538/49538\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 4ms/step - accuracy: 0.9710 - loss: 0.0738 - precision_2: 0.8840 - recall_2: 0.9822 - val_accuracy: 0.9707 - val_loss: 0.0651 - val_precision_2: 0.8795 - val_recall_2: 0.9866\n",
            "Restoring model weights from the end of the best epoch: 5.\n",
            "--- Model Training Finished ---\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "y = y.astype(int)\n",
        "\n",
        "weights = class_weight.compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
        "class_weights = dict(enumerate(weights))\n",
        "\n",
        "print(f\"\\nCalculated class weights: {class_weights}\")\n",
        "print(\"This will penalize errors on the minority class more heavily.\")\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_scaled, y,\n",
        "    test_size=0.2, \n",
        "    random_state=42,\n",
        "    stratify=y \n",
        ")\n",
        "\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  \n",
        "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy', \n",
        "    metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss', \n",
        "    patience=5,          \n",
        "    verbose=1,           \n",
        "    restore_best_weights=True \n",
        ")\n",
        "\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    filepath='best_model.h5',\n",
        "    monitor='val_loss',      \n",
        "    save_best_only=True,    \n",
        "    verbose=1               \n",
        ")\n",
        "\n",
        "\n",
        "print(\"\\n--- Starting Model Training with Early Stopping and Checkpointing ---\")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=5, \n",
        "    batch_size=32,\n",
        "    validation_data=(X_val, y_val),\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[early_stopping, model_checkpoint],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"--- Model Training Finished ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZ0ZQN1gI7Qg",
        "outputId": "812b0e72-70ee-46db-ed51-9300fee30c3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m12385/12385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step\n",
            "\n",
            "--- Model Evaluation on Validation Set ---\n",
            "Confusion Matrix:\n",
            "[[307679  10555]\n",
            " [  1044  77026]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.97      0.98    318234\n",
            "           1       0.88      0.99      0.93     78070\n",
            "\n",
            "    accuracy                           0.97    396304\n",
            "   macro avg       0.94      0.98      0.96    396304\n",
            "weighted avg       0.97      0.97      0.97    396304\n",
            "\n"
          ]
        }
      ],
      "source": [
        "y_pred_proba = model.predict(X_val)\n",
        "y_pred = (y_pred_proba > 0.5).astype(int)\n",
        "\n",
        "print(\"\\n--- Model Evaluation on Validation Set ---\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_val, y_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_val, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSN-LdkINdSz",
        "outputId": "ea6ebd82-45ad-4bb3-aca7-8bbf8ded07b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Hold-out test set loaded successfully.\n",
            "--- Applying preprocessing to the test set ---\n",
            "✅ Test set preprocessed identically.\n",
            "\n",
            "--- Generating Final Performance Report ---\n",
            "\u001b[1m26539/26539\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 1ms/step\n",
            "\n",
            "FINAL Classification Report on Unseen Test Data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.97      0.98    681929\n",
            "           1       0.88      0.99      0.93    167294\n",
            "\n",
            "    accuracy                           0.97    849223\n",
            "   macro avg       0.94      0.98      0.96    849223\n",
            "weighted avg       0.97      0.97      0.97    849223\n",
            "\n",
            "\n",
            "FINAL Confusion Matrix on Unseen Test Data:\n",
            "[[659897  22032]\n",
            " [  2287 165007]]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "try:\n",
        "    test_df = pd.read_csv('test_set.csv')\n",
        "    print(\"✅ Hold-out test set loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"❌ Error: 'testing_set.csv' not found. Please check the file name.\")\n",
        "    exit()\n",
        "\n",
        "# --- 2. Separate Features and Labels ---\n",
        "X_test = test_df.drop(' Label', axis=1)\n",
        "y_test = test_df[' Label']\n",
        "\n",
        "\n",
        "# --- 3. Apply IDENTICAL Preprocessing Steps ---\n",
        "print(\"--- Applying preprocessing to the test set ---\")\n",
        "\n",
        "# a. Align columns using the columns from the ORIGINAL training DataFrame 'X'\n",
        "# This is the fix for your AttributeError.\n",
        "train_cols = X.columns\n",
        "X_test = X_test.reindex(columns=train_cols, fill_value=0)\n",
        "\n",
        "# b. Clean any infinite values and fill NaNs\n",
        "X_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "# Use the median calculated from the TRAINING data's DataFrame 'X'\n",
        "X_test.fillna(X.median(), inplace=True)\n",
        "\n",
        "# c. Normalize the features using the SAME scaler fitted during training\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"✅ Test set preprocessed identically.\")\n",
        "\n",
        "\n",
        "# --- 4. Make Predictions and Evaluate ---\n",
        "print(\"\\n--- Generating Final Performance Report ---\")\n",
        "\n",
        "# Use the final, trained 'model' to predict on the prepared test data\n",
        "y_test_pred_proba = model.predict(X_test_scaled)\n",
        "y_test_pred = (y_test_pred_proba > 0.5).astype(int)\n",
        "\n",
        "# Print the final classification report\n",
        "print(\"\\nFINAL Classification Report on Unseen Test Data:\")\n",
        "print(classification_report(y_test, y_test_pred))\n",
        "\n",
        "# Print the final confusion matrix\n",
        "print(\"\\nFINAL Confusion Matrix on Unseen Test Data:\")\n",
        "print(confusion_matrix(y_test, y_test_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xV2IvFfPVF0L",
        "outputId": "9d29dc01-7d5a-47c6-8ec9-bb138afe87a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ GPU is available. TensorFlow will automatically use it.\n",
            "   - PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# List all physical devices visible to TensorFlow\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "\n",
        "if gpus:\n",
        "    print(f\"✅ GPU is available. TensorFlow will automatically use it.\")\n",
        "    for gpu in gpus:\n",
        "        print(f\"   - {gpu}\")\n",
        "else:\n",
        "    print(\"⚠️ No GPU found. TensorFlow will use the CPU.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6mu5dGMdzKv"
      },
      "source": [
        "### SECOND MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IU1-JwagZIFA",
        "outputId": "c1a0de87-51af-4b07-a1ef-c3870705e446"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Dataset loaded successfully.\n",
            "Dataset shape: (557646, 79)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.utils import class_weight\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Load your dataset\n",
        "# Replace 'your_multiclass_dataset.csv' with the name of your file\n",
        "try:\n",
        "    df = pd.read_csv('secondary_classification_dataset.csv')\n",
        "    print(\"✅ Dataset loaded successfully.\")\n",
        "    print(f\"Dataset shape: {df.shape}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"❌ Error: 'your_multiclass_dataset.csv' not found. Please check the file name.\")\n",
        "    exit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "id": "UGSXylhRgJg3",
        "outputId": "a150ece2-8bf3-475c-cab9-4dc010385cc0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Label</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>DoS Hulk</th>\n",
              "      <td>231073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PortScan</th>\n",
              "      <td>158930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DDoS</th>\n",
              "      <td>128027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DoS GoldenEye</th>\n",
              "      <td>10293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>FTP-Patator</th>\n",
              "      <td>7938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SSH-Patator</th>\n",
              "      <td>5897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DoS slowloris</th>\n",
              "      <td>5796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DoS Slowhttptest</th>\n",
              "      <td>5499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Bot</th>\n",
              "      <td>1966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Web Attack � Brute Force</th>\n",
              "      <td>1507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Web Attack � XSS</th>\n",
              "      <td>652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Infiltration</th>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Web Attack � Sql Injection</th>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Heartbleed</th>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ],
            "text/plain": [
              " Label\n",
              "DoS Hulk                      231073\n",
              "PortScan                      158930\n",
              "DDoS                          128027\n",
              "DoS GoldenEye                  10293\n",
              "FTP-Patator                     7938\n",
              "SSH-Patator                     5897\n",
              "DoS slowloris                   5796\n",
              "DoS Slowhttptest                5499\n",
              "Bot                             1966\n",
              "Web Attack � Brute Force        1507\n",
              "Web Attack � XSS                 652\n",
              "Infiltration                      36\n",
              "Web Attack � Sql Injection        21\n",
              "Heartbleed                        11\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[' Label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQ1j1iWkhW8c",
        "outputId": "a42f11ed-2caa-499a-875a-a4f68733a691"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Original Class Distribution ---\n",
            " Label\n",
            "DoS Hulk                      231073\n",
            "PortScan                      158930\n",
            "DDoS                          128027\n",
            "DoS GoldenEye                  10293\n",
            "FTP-Patator                     7938\n",
            "SSH-Patator                     5897\n",
            "DoS slowloris                   5796\n",
            "DoS Slowhttptest                5499\n",
            "Bot                             1966\n",
            "Web Attack � Brute Force        1507\n",
            "Web Attack � XSS                 652\n",
            "Rare_Attack                       47\n",
            "Web Attack � Sql Injection        21\n",
            "Name: count, dtype: int64\n",
            "\n",
            "--- New Class Distribution After Grouping ---\n",
            " Label\n",
            "DoS Hulk            231073\n",
            "PortScan            158930\n",
            "DDoS                128027\n",
            "DoS GoldenEye        10293\n",
            "FTP-Patator           7938\n",
            "SSH-Patator           5897\n",
            "DoS slowloris         5796\n",
            "DoS Slowhttptest      5499\n",
            "Web_Attack            2180\n",
            "Bot                   1966\n",
            "Rare_Attack             47\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(\"--- Original Class Distribution ---\")\n",
        "print(df[' Label'].value_counts())\n",
        "\n",
        "# Define the mapping for the classes you want to group\n",
        "# We'll map the specific web attacks to a general 'Web_Attack' category\n",
        "web_attack_map = {\n",
        "    'Web Attack � Brute Force': 'Web_Attack',\n",
        "    'Web Attack � XSS': 'Web_Attack',\n",
        "    'Web Attack � Sql Injection': 'Web_Attack'\n",
        "}\n",
        "\n",
        "# You can also create a map for very rare attacks\n",
        "rare_attack_map = {\n",
        "    'Infiltration': 'Rare_Attack',\n",
        "    'Heartbleed': 'Rare_Attack'\n",
        "}\n",
        "\n",
        "# Apply the replacements to the 'Label1' column\n",
        "df[' Label'] = df[' Label'].replace(web_attack_map)\n",
        "df[' Label'] = df[' Label'].replace(rare_attack_map)\n",
        "\n",
        "print(\"\\n--- New Class Distribution After Grouping ---\")\n",
        "print(df[' Label'].value_counts())\n",
        "\n",
        "# Now you can proceed with preprocessing this modified DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1MI_8DXi8iW",
        "outputId": "8a33eb4a-78a4-4306-bb5f-b55449eb5604"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Distribution Before Final Grouping ---\n",
            " Label\n",
            "DoS Hulk            231073\n",
            "PortScan            158930\n",
            "DDoS                128027\n",
            "DoS GoldenEye        10293\n",
            "FTP-Patator           7938\n",
            "SSH-Patator           5897\n",
            "DoS slowloris         5796\n",
            "DoS Slowhttptest      5499\n",
            "Web_Attack            2180\n",
            "Bot                   1966\n",
            "Rare_Attack             47\n",
            "Name: count, dtype: int64\n",
            "\n",
            "--- Final, Improved Class Distribution ---\n",
            " Label\n",
            "DoS_Attack      380688\n",
            "PortScan        158930\n",
            "Brute_Force      13835\n",
            "Web_Attack        2180\n",
            "Other_Attack      2013\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assume 'df' is your DataFrame after the first round of grouping\n",
        "# with the ' Label' column.\n",
        "\n",
        "print(\"--- Distribution Before Final Grouping ---\")\n",
        "print(df[' Label'].value_counts())\n",
        "\n",
        "# Define the new mapping dictionaries\n",
        "dos_map = {\n",
        "    'DoS Hulk': 'DoS_Attack',\n",
        "    'DDoS': 'DoS_Attack',\n",
        "    'DoS GoldenEye': 'DoS_Attack',\n",
        "    'DoS slowloris': 'DoS_Attack',\n",
        "    'DoS Slowhttptest': 'DoS_Attack'\n",
        "}\n",
        "\n",
        "brute_force_map = {\n",
        "    'FTP-Patator': 'Brute_Force',\n",
        "    'SSH-Patator': 'Brute_Force'\n",
        "}\n",
        "\n",
        "other_map = {\n",
        "    'Bot': 'Other_Attack',\n",
        "    'Rare_Attack': 'Other_Attack'\n",
        "}\n",
        "\n",
        "# Apply the replacements sequentially\n",
        "df[' Label'] = df[' Label'].replace(dos_map)\n",
        "df[' Label'] = df[' Label'].replace(brute_force_map)\n",
        "df[' Label'] = df[' Label'].replace(other_map)\n",
        "\n",
        "print(\"\\n--- Final, Improved Class Distribution ---\")\n",
        "print(df[' Label'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y90xw42AewKN",
        "outputId": "a7a37e7c-8ae5-4bfb-ffaa-7a2aa5589b96"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1765854896.py:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[col].fillna(df[col].median(), inplace=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Missing values handled.\n",
            "Found 5 unique classes.\n"
          ]
        }
      ],
      "source": [
        "# --- Data Cleaning ---\n",
        "# Fill missing numerical values with the column median\n",
        "for col in df.select_dtypes(include=np.number).columns:\n",
        "    df[col].fillna(df[col].median(), inplace=True)\n",
        "print(\"✅ Missing values handled.\")\n",
        "\n",
        "# --- Separate Features (X) and Labels (y) ---\n",
        "X = df.drop(' Label', axis=1)\n",
        "y = df[' Label']\n",
        "\n",
        "# --- Label Encoding for Multiclass ---\n",
        "# First, convert text labels to integer labels if they aren't already\n",
        "y, class_names = pd.factorize(y)\n",
        "num_classes = len(class_names)\n",
        "print(f\"Found {num_classes} unique classes.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Cf0pekijaHs",
        "outputId": "86d4e618-cf7c-4c22-ab0f-7eb753a2ac07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 5 unique classes.\n",
            "✅ Labels have been one-hot encoded.\n"
          ]
        }
      ],
      "source": [
        "# --- Label Encoding for Multiclass ---\n",
        "# First, convert text labels to integer labels if they aren't already\n",
        "y, class_names = pd.factorize(y)\n",
        "num_classes = len(class_names)\n",
        "print(f\"Found {num_classes} unique classes.\")\n",
        "\n",
        "# Second, one-hot encode the integer labels\n",
        "# This converts a single column of labels [0, 1, 2] into three columns:\n",
        "# [[1,0,0], [0,1,0], [0,0,1]]\n",
        "y_one_hot = tf.keras.utils.to_categorical(y, num_classes=num_classes)\n",
        "print(\"✅ Labels have been one-hot encoded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyH_resKg-Rv",
        "outputId": "ac2cf5e9-cdcb-4777-8b72-c1f758a91d07"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2633925225.py:14: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X[col].fillna(X[col].median(), inplace=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Infinite and missing values in features handled.\n",
            "✅ Features have been normalized using Min-Max scaling.\n",
            "Calculated class weights: {0: np.float64(0.2929674694237801), 1: np.float64(0.7017504561756748), 2: np.float64(55.40447093889717), 3: np.float64(51.16018348623853), 4: np.float64(8.06138055655945)}\n",
            "Data split into 446116 training and 111530 validation samples.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler # Import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "import numpy as np\n",
        "\n",
        "# --- Data Cleaning: Handle infinities and NaNs in X ---\n",
        "# Replace infinite values with NaN\n",
        "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "# Fill remaining NaN values with the median of each column\n",
        "# (Using the median is robust to outliers which might be present due to large values)\n",
        "for col in X.columns:\n",
        "    if X[col].isnull().any():\n",
        "        X[col].fillna(X[col].median(), inplace=True)\n",
        "print(\"✅ Infinite and missing values in features handled.\")\n",
        "\n",
        "\n",
        "# --- Feature Scaling ---\n",
        "# Apply Min-Max Scaling instead of Standard Scaling\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "print(\"✅ Features have been normalized using Min-Max scaling.\")\n",
        "\n",
        "# --- Handle Class Imbalance ---\n",
        "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
        "class_weights_dict = dict(enumerate(class_weights))\n",
        "print(f\"Calculated class weights: {class_weights_dict}\")\n",
        "\n",
        "# --- Create Training and Validation Sets ---\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_scaled, y_one_hot,\n",
        "    test_size=0.2, # 20% for validation\n",
        "    random_state=42,\n",
        "    stratify=y # Stratify based on the original integer labels\n",
        ")\n",
        "print(f\"Data split into {len(X_train)} training and {len(X_val)} validation samples.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-PYnYfzjkDO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 838
        },
        "id": "PxbpNIbGjre1",
        "outputId": "d347d8ad-8e03-4479-98d4-28be27aa68f1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,112</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">325</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m10,112\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │           \u001b[38;5;34m325\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18,693</span> (73.02 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m18,693\u001b[0m (73.02 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18,693</span> (73.02 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m18,693\u001b[0m (73.02 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Starting Model Training ---\n",
            "Epoch 1/20\n",
            "\u001b[1m13942/13942\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9561 - loss: 0.4542 - precision: 0.9740 - recall: 0.9005"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m13942/13942\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 4ms/step - accuracy: 0.9561 - loss: 0.4542 - precision: 0.9740 - recall: 0.9005 - val_accuracy: 0.9911 - val_loss: 0.0643 - val_precision: 0.9926 - val_recall: 0.9908\n",
            "Epoch 2/20\n",
            "\u001b[1m13941/13942\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9892 - loss: 0.1028 - precision: 0.9906 - recall: 0.9876"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m13942/13942\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 3ms/step - accuracy: 0.9892 - loss: 0.1028 - precision: 0.9906 - recall: 0.9876 - val_accuracy: 0.9952 - val_loss: 0.0383 - val_precision: 0.9954 - val_recall: 0.9951\n",
            "Epoch 3/20\n",
            "\u001b[1m13935/13942\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9908 - loss: 0.0947 - precision: 0.9917 - recall: 0.9895"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m13942/13942\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 3ms/step - accuracy: 0.9908 - loss: 0.0946 - precision: 0.9917 - recall: 0.9895 - val_accuracy: 0.9962 - val_loss: 0.0234 - val_precision: 0.9962 - val_recall: 0.9958\n",
            "Epoch 4/20\n",
            "\u001b[1m13942/13942\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 3ms/step - accuracy: 0.9911 - loss: 0.0977 - precision: 0.9919 - recall: 0.9901 - val_accuracy: 0.9953 - val_loss: 0.0363 - val_precision: 0.9955 - val_recall: 0.9953\n",
            "Epoch 5/20\n",
            "\u001b[1m13939/13942\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9891 - loss: 0.0967 - precision: 0.9906 - recall: 0.9871"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m13942/13942\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 3ms/step - accuracy: 0.9891 - loss: 0.0967 - precision: 0.9906 - recall: 0.9871 - val_accuracy: 0.9968 - val_loss: 0.0142 - val_precision: 0.9971 - val_recall: 0.9967\n",
            "Epoch 6/20\n",
            "\u001b[1m13936/13942\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9910 - loss: 0.0993 - precision: 0.9921 - recall: 0.9899"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m13942/13942\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 3ms/step - accuracy: 0.9910 - loss: 0.0993 - precision: 0.9921 - recall: 0.9899 - val_accuracy: 0.9964 - val_loss: 0.0142 - val_precision: 0.9965 - val_recall: 0.9962\n",
            "Epoch 7/20\n",
            "\u001b[1m13942/13942\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 3ms/step - accuracy: 0.9896 - loss: 0.0958 - precision: 0.9912 - recall: 0.9879 - val_accuracy: 0.9954 - val_loss: 0.0317 - val_precision: 0.9957 - val_recall: 0.9954\n",
            "Epoch 8/20\n",
            "\u001b[1m13942/13942\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 3ms/step - accuracy: 0.9894 - loss: 0.1399 - precision: 0.9904 - recall: 0.9883 - val_accuracy: 0.9959 - val_loss: 0.0251 - val_precision: 0.9960 - val_recall: 0.9954\n",
            "Epoch 9/20\n",
            "\u001b[1m13942/13942\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 3ms/step - accuracy: 0.9914 - loss: 0.0770 - precision: 0.9926 - recall: 0.9901 - val_accuracy: 0.9964 - val_loss: 0.0158 - val_precision: 0.9969 - val_recall: 0.9963\n",
            "Epoch 10/20\n",
            "\u001b[1m13942/13942\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 3ms/step - accuracy: 0.9911 - loss: 0.0828 - precision: 0.9926 - recall: 0.9895 - val_accuracy: 0.9964 - val_loss: 0.0145 - val_precision: 0.9964 - val_recall: 0.9963\n",
            "Epoch 11/20\n",
            "\u001b[1m13942/13942\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 3ms/step - accuracy: 0.9896 - loss: 0.1117 - precision: 0.9911 - recall: 0.9882 - val_accuracy: 0.9971 - val_loss: 0.0149 - val_precision: 0.9973 - val_recall: 0.9970\n",
            "--- Model Training Finished ---\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# --- Build the Neural Network ---\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    # KEY CHANGE: Output layer for multiclass\n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# --- Compile the Model ---\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    # ADD PRECISION AND RECALL TO THE METRICS LIST\n",
        "    metrics=[\n",
        "        'accuracy',\n",
        "        tf.keras.metrics.Precision(),\n",
        "        tf.keras.metrics.Recall()\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# --- Define Callbacks for Efficient Training ---\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "model_checkpoint = ModelCheckpoint(filepath='best_multiclass_model.h5', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "# --- Train the Model ---\n",
        "print(\"\\n--- Starting Model Training ---\")\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=20,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_val, y_val),\n",
        "    class_weight=class_weights_dict,\n",
        "    callbacks=[early_stopping, model_checkpoint],\n",
        "    verbose=1\n",
        ")\n",
        "print(\"--- Model Training Finished ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeiLDnebkdDH",
        "outputId": "df3a732d-56cd-4199-aec4-182626ec3087"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m3486/3486\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n",
            "\n",
            "--- Model Evaluation on Validation Set ---\n",
            "Confusion Matrix:\n",
            "[[75862     1     2   124   149]\n",
            " [   60 31722     0     4     0]\n",
            " [    1     0   401     0     1]\n",
            " [    6     0     0   391    39]\n",
            " [    6     2     0     2  2757]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  DoS_Attack       1.00      1.00      1.00     76138\n",
            "    PortScan       1.00      1.00      1.00     31786\n",
            "Other_Attack       1.00      1.00      1.00       403\n",
            "  Web_Attack       0.75      0.90      0.82       436\n",
            " Brute_Force       0.94      1.00      0.97      2767\n",
            "\n",
            "    accuracy                           1.00    111530\n",
            "   macro avg       0.94      0.98      0.95    111530\n",
            "weighted avg       1.00      1.00      1.00    111530\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# --- Evaluate the Model ---\n",
        "# The model outputs probabilities for each class\n",
        "y_pred_proba = model.predict(X_val)\n",
        "\n",
        "# To get the final predicted class, we find the index of the highest probability\n",
        "y_pred = np.argmax(y_pred_proba, axis=1)\n",
        "\n",
        "# The validation labels are one-hot encoded, so we convert them back for the report\n",
        "y_true = np.argmax(y_val, axis=1)\n",
        "\n",
        "# --- Print Evaluation Metrics ---\n",
        "print(\"\\n--- Model Evaluation on Validation Set ---\")\n",
        "\n",
        "# 1. Confusion Matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "# 2. Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=class_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3s4Pl8KVnr9q",
        "outputId": "2f9bfc75-d2b2-499b-a6f3-bf48311dd2af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Model saved successfully to 'my_multiclass_model.keras'\n"
          ]
        }
      ],
      "source": [
        "# Assume 'model' is your trained Keras model\n",
        "model.save('my_multiclass_model.keras')\n",
        "\n",
        "print(\"✅ Model saved successfully to 'my_multiclass_model.keras'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "101ea245",
        "outputId": "9efc5a33-a557-4acd-9e29-421eec71d5f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 190788\n",
            "-rw-r--r-- 1 root root    257736 Oct 11 14:03 best_multiclass_model.h5\n",
            "-rw-r--r-- 1 root root    253995 Oct 11 14:13 my_multiclass_model.keras\n",
            "drwxr-xr-x 1 root root      4096 Oct  9 13:36 sample_data\n",
            "-rw-r--r-- 1 root root 194841054 Oct 11 13:37 secondary_classification_dataset.csv\n"
          ]
        }
      ],
      "source": [
        "!ls -l"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
